#!/usr/bin/python2

import rospy
from sensor_msgs.msg import LaserScan
from geometry_msgs.msg import Pose2D 
import numpy as np

to_right = 0
in_front = 0
to_left = 0


def callback(msg):
    to_right = msg.ranges[0]
    in_front = msg.ranges[90]
    to_left = msg.ranges[180]

# initialize publisher and subscribers
# rate = rospy.Rate(1)



PI = 3.14159
# am probably not going to need eventually

def discrete(distance):
    if distance <= 0.5:
        return 0 # too close
    if (distance > 0.5) and (distance <= 1.2):
        return 1 # just right
    else:
        return 2 # too far 

def get_discrete_state():
    rospy.init_node('scan_values', anonymous=True)
    sub = rospy.Subscriber('/scan', LaserScan, callback)
    minimum_wall = min(to_right, in_front, to_left)
    if min == to_right:
        wall_distance = discrete(to_right)
        wall_orientation = 0 # nearest wall is to the right
    elif min == in_front:
        wall_distance = discrete(in_front)
        wall_orientation = 1
    else:
        wall_distance = discrete(to_left)
        wall_orientation = 2
    return (wall_orientation, wall_distance);


# calls /gazebo/reset_simulation service
def reset_simulation():
    rospy.wait_for_service('/gazebo/reset_simulation')
    try:
        reset_simulation = rospy.ServiceProxy('/gazebo/reset_simulation', ResetSim)
        doIt = reset_simulation()
    except rospy.ServiceException, e:
        print "Service call failed: %s"%e
        

def step(action):
    
    pub = rospy.Publisher('/triton_lidar/vel_cmd', Pose2D, queue_size=10)
    rospy.init_node('mover', anonymous=True)

    # declare a publisher to move the robot
    new_pose = Pose2D()
    if action == 0: # forward
        new_pose.x = 3
    elif action == 1: # left
        new_pose.theta = PI / 2
        new_pose.x = 3
        new_pose.y = 3
        new_pose.theta = 0
    else: # right
        new_pose.theta = PI / 2
        new_pose.x = 3
    rate = rospy.Rate(10)
    while not rospy.is_shutdown():
        pub.publish(new_pose)
        rate.sleep()
    # put the action in a loop i think 
    # perform the action
    done = False
    reward = 0
    return(List[get_discrete_state(), reward, done])

LEARNING_RATE = 0.1 # from 0 - 1
DISCOUNT = 0.95 # measure of how important future actions are
EPISODES = 150 
STEPS = 1000

DISCRETE_OS_SIZE = [3] * 3  
ACTION_SIZE = 3 # move forward, left, right
epsilon = 0.5 # measure of randomness

START_EPSILON_DECAYING = 1
END_EPSILON_DECAYING = EPISODES // 2
epsilon_decay_value = epsilon / (END_EPSILON_DECAYING - START_EPSILON_DECAYING)

# start with random q values 
q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [ACTION_SIZE])) 


for episode in range(EPISODES):
    # TODO: reset robot position maybe action(action)
    # reset_simulation()
    discrete_state = get_discrete_state()  # pass beginning state of robot
    done = False
    steps = STEPS
    while not done:
        if np.random.random() > epsilon:
            action = np.argmax(q_table[discrete_state])
        else:
            action = np.random.randint(0, ACTION_SIZE)

        # take a step and update some variables  
        funcReturn = step(action)
        new_state = funcReturn[0]
        reward = funcReturn[1]
        done = funcReturn[0]
        
        
        # if render is true, render
        if not done:
            max_future_q = np.max(q_table[new_discrete_state])
            current_q = q_table[discrete_state + (action, )]
            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)
            q_table[discrete_state+(action, )] = new_q # update action we just took
            discrete_state = new_discrete_state
            """
        # measure of randomness 
        if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:
            epsilon -= epsilon_decay_value
        # elif new_state[0] >= goal position:
            # q_table[discrete_state + (action,)]

            """
        steps = steps - 1
        if steps == 0:
            done = True 



